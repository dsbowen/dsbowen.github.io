---
permalink: /about/
title: "About"
---

## Background

I graduated from Tufts University in 2016 with a bachelor's in cognitive science and philosophy. In 2017-2018, I earned a graduate diploma and master's in economics from the University of Cambridge. After the fourth year of my PhD at the Wharton School of Business, I'm taking a leave of absence to work at <a href="ravio.com" target="_blank">Ravio</a>, a compensation benchmarking startup.

## Research

I research statistics and experimental methods. Specifically, I study *multiple inference* - the problem of comparing many "things" at once. While the statistical and experimental tools I've developed apply to many scientific fields, I'm most interested in applying them to forecasting. For example, how might we compare the predictive accuracy of many forecasters to assemble a team of *superforecasters*? Or, how should we run forecasting tournaments testing the effectiveness of many interventions to improve accuracy and persuasion?

Research highlights:

- <a href="https://arxiv.org/abs/2208.01167" target="_blank">Simple models</a> forecast behavior at least as well as professional behavioral scientists.
- <a href="/download/megastudies.pdf" target="_blank">Megastudies</a> - in which researchers test many treatments in a single, large-scale study - haven't been very effective. They only *appear* effective because of statistical errors. To run megastudies more effectively - for example, a forecasting tournament testing many interventions to improve accuracy - researchers should use adaptive assignment. If that sounds difficult, <a href="https://dsbowen.gitlab.io/hemlock/" target="_blank">here</a> is a Qualtrics-like software I designed to help.
<!-- - <a href="URL" target="_blank">A social scientist's guide</a> to multiple inference with accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.04492" target="_blank">statistics package</a>. For example, suppose we run a forecasting tournament testing many interventions to improve accuracy and want to understand which ones are the most effective and by how much. -->
- <a href="https://swlb1.aeaweb.org/articles?id=10.1257/pandp.20221065" target="_blank">A new statistical estimator</a> for inference after ranking. For example, how accurate are the top-performing forecasters in a forecasting tournament?
<!-- - <a href="URL" target="_blank">New Bayesian algorithms</a> for ranking and selection. For example, suppose we want to assemble a team of forecasters who we are confident rank among the top 10% in forecasting ability. -->